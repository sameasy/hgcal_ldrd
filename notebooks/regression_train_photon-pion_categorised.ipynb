{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from scipy.spatial import cKDTree\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "from datasets.graph import draw_sample\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch\n",
    "import os\n",
    "import os.path as osp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import photon clusters\n",
    "import glob\n",
    "raw_dir= '/home/sameasy2006/npz_hgcal_50k_k8/HGCNTUP_photon_3to1000_50k/clusters/'\n",
    "fnamelist = [filepath for filepath in glob.glob(raw_dir+'data_*.pt')]\n",
    "data_list_pho = []\n",
    "for i in tqdm(fnamelist):\n",
    "    idx = torch.load(i)\n",
    "    #print(idx.y)\n",
    "    idx.y = np.log(idx.y)\n",
    "    #idx.w = phoweight\n",
    "    idx.z = torch.tensor([[0,idx.y]])\n",
    "    #print(idx.y)\n",
    "    data_list_pho.append(idx)\n",
    "    #data_list.append(torch.load(i))\n",
    "    \n",
    "\n",
    "#data_list=[x for x in data_list if (x.y<100.)]   \n",
    "print((data_list_pho[0].y))\n",
    "print((data_list_pho[0].z))\n",
    "print(data_list_pho[0].pos)\n",
    "print(len(data_list_pho))\n",
    "totalevpho = len(data_list_pho)\n",
    "#trainev = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pion clusters\n",
    "import glob\n",
    "raw_dir='/home/sameasy2006/npz_hgcal_pion_50k_k8/HGCNTUP_pion_3to1000_50k/clusters/'\n",
    "fnamelist = [filepath for filepath in glob.glob(raw_dir+'data_*.pt')]\n",
    "data_list_pi = []\n",
    "\n",
    "for i in tqdm(fnamelist):\n",
    "    idx = torch.load(i)\n",
    "    #print(idx.y)\n",
    "    idx.y = np.log(idx.y)\n",
    "    #idx.w = piweight\n",
    "    idx.z = torch.tensor([[1,idx.y]])\n",
    "    #print(idx.y)\n",
    "    data_list_pi.append(idx)\n",
    "    #data_list.append(torch.load(i))\n",
    "    \n",
    "#data_list=[x for x in data_list if (x.y > 800.)]     \n",
    "print((data_list_pi[0].y))\n",
    "#print((data_list_pi[0].x))\n",
    "print((data_list_pi[0].z))\n",
    "print(data_list_pi[0].pos)\n",
    "print(len(data_list_pi))\n",
    "totalevpi = len(data_list_pi)\n",
    "#trainev = \n",
    "#print((data_list_pi[0].batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del data_list_comb\n",
    "import random\n",
    "\n",
    "data_list_comb = data_list_pho + data_list_pi[:totalevpho]\n",
    "#print(data_list_comb[0])\n",
    "random.shuffle(data_list_comb)\n",
    "#print(data_list_comb[0])\n",
    "totalev = len(data_list_comb)\n",
    "print(\"total comb evs:\",totalev)\n",
    "import torch_geometric\n",
    "ntrainbatch = 50\n",
    "ntestbatch = 20\n",
    "trainloader = torch_geometric.data.DataLoader(data_list_comb[:totalev-500], batch_size=ntrainbatch)\n",
    "testloader = torch_geometric.data.DataLoader(data_list_comb[totalev-500:totalev], batch_size=ntestbatch)\n",
    "#batch_size = ntrainbatch\n",
    "epoch_size = len(data_list_comb[:totalev-500])\n",
    "print(\"epoch size,batch_size:\",epoch_size,ntrainbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "from torch_geometric.nn import EdgeConv, NNConv\n",
    "#from torch_geometric.nn.pool.edge_pool import EdgePooling\n",
    "\n",
    "from torch_geometric.utils import normalized_cut\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "from torch_geometric.utils.undirected import to_undirected\n",
    "from torch_geometric.nn import (graclus, max_pool, max_pool_x,\n",
    "                                global_mean_pool, global_max_pool,\n",
    "                                global_add_pool)\n",
    "\n",
    "transform = T.Cartesian(cat=False)\n",
    "\n",
    "def normalized_cut_2d(edge_index, pos):\n",
    "    row, col = edge_index\n",
    "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
    "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))\n",
    "\n",
    "class DynamicReductionNetwork(nn.Module):\n",
    "    # This model iteratively contracts nearest neighbour graphs \n",
    "    # until there is one output node.\n",
    "    # The latent space trained to group useful features at each level\n",
    "    # of aggregration.\n",
    "    # This allows single quantities to be regressed from complex point counts\n",
    "    # in a location and orientation invariant way.\n",
    "    # One encoding layer is used to abstract away the input features.\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1, k=16, aggr='add',\n",
    "                 norm=torch.tensor([1./500., 1./500., 1./54., 1/25., 1./1000.])):\n",
    " #                norm=torch.tensor([1., 1., 1., 1., 1.])):\n",
    "        super(DynamicReductionNetwork, self).__init__()\n",
    "\n",
    "        self.datanorm = nn.Parameter(norm)\n",
    "        \n",
    "        self.k = k\n",
    "        start_width = 2 * hidden_dim\n",
    "        middle_width = 3 * hidden_dim // 2\n",
    "\n",
    "        \n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim*2),            \n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim*2),\n",
    "            nn.ELU(),\n",
    "#            nn.Linear(hidden_dim*2, hidden_dim*2),\n",
    "#            nn.ELU(),\n",
    "#            nn.Linear(hidden_dim*2, hidden_dim*2),\n",
    "#            nn.ELU(),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        convnn1 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                                nn.ELU()\n",
    "                                )\n",
    "        convnn2 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                                nn.ELU()\n",
    "                                )\n",
    "        \n",
    "        convnn3 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                                nn.ELU()\n",
    "                                )\n",
    "                \n",
    "        self.edgeconv1 = EdgeConv(nn=convnn1, aggr=aggr)\n",
    "        self.edgeconv2 = EdgeConv(nn=convnn2, aggr=aggr)\n",
    "        self.edgeconv3 = EdgeConv(nn=convnn3, aggr=aggr)\n",
    "        \n",
    "        self.output = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ELU(),\n",
    "                                    #nn.Softplus(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                    nn.ELU(),\n",
    "                                    #nn.Softplus(),\n",
    "#                                    nn.Linear(hidden_dim//2, hidden_dim//2),#added\n",
    " #                                   nn.ELU(),\n",
    "                                    #nn.Softplus(),\n",
    "                                    nn.Linear(hidden_dim//2, output_dim)\n",
    "                                   )\n",
    "        \n",
    "        \n",
    "    def forward(self, data):        \n",
    "        data.x = self.datanorm * data.x\n",
    "        data.x = self.inputnet(data.x)\n",
    "        \n",
    "        #print(data.batch)\n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv1.flow))\n",
    "        data.x = self.edgeconv1(data.x, data.edge_index)\n",
    "        \n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        data.edge_attr = None\n",
    "        data = max_pool(cluster, data)\n",
    "        \n",
    "        ####\n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv3.flow))\n",
    "        data.x = self.edgeconv3(data.x, data.edge_index)\n",
    "        \n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        data.edge_attr = None\n",
    "        data = max_pool(cluster, data)\n",
    "        ####\n",
    "        ####\n",
    "        #data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv1.flow))\n",
    "        #data.x = self.edgeconv3(data.x, data.edge_index)\n",
    "        \n",
    "        #weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        #cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        #data.edge_attr = None\n",
    "        #data = max_pool(cluster, data)\n",
    "        ####\n",
    "        \n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv2.flow))\n",
    "        data.x = self.edgeconv2(data.x, data.edge_index)\n",
    "        \n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        x, batch = max_pool_x(cluster, data.x, data.batch)\n",
    "\n",
    "        x = global_max_pool(x, batch)\n",
    "#        print(self.output(x))\n",
    "        return self.output(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "class ReduceMaxLROnRestart:\n",
    "    def __init__(self, ratio=0.75):\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        def __call__(self, eta_min, eta_max):\n",
    "            return eta_min, eta_max * self.ratio\n",
    "        \n",
    "        \n",
    "class ExpReduceMaxLROnIteration:\n",
    "    def __init__(self, gamma=1):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __call__(self, eta_min, eta_max, iterations):\n",
    "        return eta_min, eta_max * self.gamma ** iterations\n",
    "\n",
    "\n",
    "class CosinePolicy:\n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        return 0.5 * (1. + math.cos(math.pi *\n",
    "                                    (t_cur / restart_period)))\n",
    "    \n",
    "    \n",
    "class ArccosinePolicy:\n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        return (math.acos(max(-1, min(1, 2 * t_cur\n",
    "                                      / restart_period - 1))) / math.pi)\n",
    "    \n",
    "    \n",
    "class TriangularPolicy:\n",
    "    def __init__(self, triangular_step=0.5):\n",
    "        self.triangular_step = triangular_step\n",
    "        \n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        inflection_point = self.triangular_step * restart_period\n",
    "        point_of_triangle = (t_cur / inflection_point\n",
    "                             if t_cur < inflection_point\n",
    "                             else 1.0 - (t_cur - inflection_point)\n",
    "                             / (restart_period - inflection_point))\n",
    "        return point_of_triangle\n",
    "    \n",
    "    \n",
    "class CyclicLRWithRestarts(_LRScheduler):\n",
    "    \"\"\"Decays learning rate with cosine annealing, normalizes weight decay\n",
    "    hyperparameter value, implements restarts.\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        batch_size: minibatch size\n",
    "        epoch_size: training samples per epoch\n",
    "        restart_period: epoch count in the first restart period\n",
    "        t_mult: multiplication factor by which the next restart period will expand/shrink\n",
    "        policy: [\"cosine\", \"arccosine\", \"triangular\", \"triangular2\", \"exp_range\"]\n",
    "        min_lr: minimum allowed learning rate\n",
    "        verbose: print a message on every restart\n",
    "        gamma: exponent used in \"exp_range\" policy\n",
    "        eta_on_restart_cb: callback executed on every restart, adjusts max or min lr\n",
    "        eta_on_iteration_cb: callback executed on every iteration, adjusts max or min lr\n",
    "        triangular_step: adjusts ratio of increasing/decreasing phases for triangular policy\n",
    "    Example:\n",
    "        >>> scheduler = CyclicLRWithRestarts(optimizer, 32, 1024, restart_period=5, t_mult=1.2)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     scheduler.step()\n",
    "        >>>     train(...)\n",
    "        >>>         ...\n",
    "        >>>         optimizer.zero_grad()\n",
    "        >>>         loss.backward()\n",
    "        >>>         optimizer.step()\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>     validate(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, batch_size, epoch_size, restart_period=100,\n",
    "                 t_mult=2, last_epoch=-1, verbose=False,\n",
    "                 policy=\"cosine\", policy_fn=None, min_lr=1e-7,\n",
    "                 eta_on_restart_cb=None, eta_on_iteration_cb=None,\n",
    "                 gamma=1.0, triangular_step=0.5):\n",
    "        \n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "                group.setdefault('minimum_lr', min_lr)\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an\"\n",
    "                                   \" optimizer\".format(i))\n",
    "                \n",
    "        self.base_lrs = [group['initial_lr'] for group\n",
    "                         in optimizer.param_groups]\n",
    "        \n",
    "        self.min_lrs = [group['minimum_lr'] for group\n",
    "                        in optimizer.param_groups]\n",
    "        \n",
    "        self.base_weight_decays = [group['weight_decay'] for group\n",
    "                                   in optimizer.param_groups]\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.eta_on_restart_cb = eta_on_restart_cb\n",
    "        self.eta_on_iteration_cb = eta_on_iteration_cb\n",
    "        if policy_fn is not None:\n",
    "            self.policy_fn = policy_fn\n",
    "        elif self.policy == \"cosine\":\n",
    "            self.policy_fn = CosinePolicy()\n",
    "        elif self.policy == \"arccosine\":\n",
    "            self.policy_fn = ArccosinePolicy()\n",
    "        elif self.policy == \"triangular\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "        elif self.policy == \"triangular2\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "            self.eta_on_restart_cb = ReduceMaxLROnRestart(ratio=0.5)\n",
    "        elif self.policy == \"exp_range\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "            self.eta_on_iteration_cb = ExpReduceMaxLROnIteration(gamma=gamma)\n",
    "            \n",
    "        self.last_epoch = last_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_size = epoch_size\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.total_iterations = 0\n",
    "        \n",
    "        self.t_mult = t_mult\n",
    "        self.verbose = verbose\n",
    "        self.restart_period = math.ceil(restart_period)\n",
    "        self.restarts = 0\n",
    "        self.t_epoch = -1\n",
    "        self.epoch = -1\n",
    "        \n",
    "        self.eta_min = 0\n",
    "        self.eta_max = 1\n",
    "        \n",
    "        self.end_of_period = False\n",
    "        self.batch_increments = []\n",
    "        self._set_batch_increment()\n",
    "        \n",
    "    def _on_restart(self):\n",
    "        if self.eta_on_restart_cb is not None:\n",
    "            self.eta_min, self.eta_max = self.eta_on_restart_cb(self.eta_min,\n",
    "                                                                self.eta_max)\n",
    "            \n",
    "    def _on_iteration(self):\n",
    "        if self.eta_on_iteration_cb is not None:\n",
    "            self.eta_min, self.eta_max = self.eta_on_iteration_cb(self.eta_min,\n",
    "                                                                  self.eta_max,\n",
    "                                                                  self.total_iterations)\n",
    "            \n",
    "    def get_lr(self, t_cur):\n",
    "        eta_t = (self.eta_min + (self.eta_max - self.eta_min)\n",
    "                 * self.policy_fn(t_cur, self.restart_period))\n",
    "        \n",
    "        weight_decay_norm_multi = math.sqrt(self.batch_size /\n",
    "                                            (self.epoch_size *\n",
    "                                             self.restart_period))\n",
    "        \n",
    "        lrs = [min_lr + (base_lr - min_lr) * eta_t for base_lr, min_lr\n",
    "               in zip(self.base_lrs, self.min_lrs)]\n",
    "        weight_decays = [base_weight_decay #* eta_t * weight_decay_norm_multi\n",
    "                         for base_weight_decay in self.base_weight_decays]\n",
    "        \n",
    "        if (self.t_epoch + 1) % self.restart_period < self.t_epoch:\n",
    "            self.end_of_period = True\n",
    "            \n",
    "        if self.t_epoch % self.restart_period < self.t_epoch:\n",
    "            if self.verbose:\n",
    "                print(\"Restart {} at epoch {}\".format(self.restarts + 1,\n",
    "                                                      self.last_epoch))\n",
    "            self.restart_period = math.ceil(self.restart_period * self.t_mult)\n",
    "            self.restarts += 1\n",
    "            self.t_epoch = 0\n",
    "            self._on_restart()\n",
    "            self.end_of_period = False\n",
    "            \n",
    "        return zip(lrs, weight_decays)\n",
    "        \n",
    "    def _set_batch_increment(self):\n",
    "        d, r = divmod(self.epoch_size, self.batch_size)\n",
    "        batches_in_epoch = d + 2 if r > 0 else d + 1\n",
    "        self.iteration = 0\n",
    "        self.batch_increments = torch.linspace(0, 1, batches_in_epoch).tolist()\n",
    "        \n",
    "    def step(self):\n",
    "        self.last_epoch += 1\n",
    "        self.t_epoch += 1\n",
    "        self._set_batch_increment()\n",
    "        self.batch_step()\n",
    "        \n",
    "    def batch_step(self):\n",
    "        try:\n",
    "            t_cur = self.t_epoch + self.batch_increments[self.iteration]\n",
    "            self._on_iteration()\n",
    "            self.iteration += 1\n",
    "            self.total_iterations += 1\n",
    "        except (IndexError):\n",
    "            raise StopIteration(\"Epoch size and batch size used in the \"\n",
    "                                \"training loop and while initializing \"\n",
    "                                \"scheduler should be the same.\")\n",
    "        \n",
    "        for param_group, (lr, weight_decay) in zip(self.optimizer.param_groups,\n",
    "                                                   self.get_lr(t_cur)):\n",
    "            param_group['lr'] = lr\n",
    "            param_group['weight_decay'] = weight_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "#from models.DynamicReductionNetwork import DynamicReductionNetwork\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drn = DynamicReductionNetwork(hidden_dim=50,k=16,output_dim=3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        logits = self.drn(data)\n",
    "        return F.softplus(logits)\n",
    "        #return logits\n",
    "device = torch.device('cuda')#('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "scheduler = CyclicLRWithRestarts(optimizer, ntrainbatch, epoch_size, restart_period=400, t_mult=1.2, policy=\"cosine\")\n",
    "#criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def resoloss2(output,truth):\n",
    "    batch_size = output.size()[0]\n",
    "#    mse = F.mse_loss(output, truth, reduction='mean')\n",
    "    res = torch.sum((output-truth)**2/truth)/batch_size\n",
    "    #return (mse + 0.2*res)\n",
    "    return (res)\n",
    "    \n",
    "\n",
    "    \n",
    "def resoloss(outputa,trutha):\n",
    "    n_bins = np.random.poisson(40) + 2\n",
    "    minx = torch.min(trutha).detach().cpu()\n",
    "    maxx = torch.max(trutha).detach().cpu()\n",
    "    minx = np.exp(minx)\n",
    "    maxx = np.exp(maxx)\n",
    "    \n",
    "    binss = np.arange(minx,maxx,(maxx-minx)/n_bins)\n",
    "    binss = np.log(binss)\n",
    "    loss = 0.\n",
    "    for i in range(binss.size-1):\n",
    "        tmpo = outputa[ (trutha > binss[i]) &  (trutha < binss[i+1])  ]\n",
    "        tmpt = trutha[ (trutha > binss[i]) &  (trutha < binss[i+1])  ]\n",
    "        #print(\"tmpo,tmpt:\",tmpo,tmpt)\n",
    "        if (list(tmpo.size())[0] == 0 or list(tmpt.size())[0] == 0):\n",
    "            continue\n",
    "        #print(list(tmpo.size())[0],list(tmpt.size())[0])\n",
    "        tmpom = torch.mean(tmpo)\n",
    "        tmptm = torch.mean(tmpt)\n",
    "        #print (tmpom,tmptm)\n",
    "        #if (tmptm == 0):\n",
    "        #    continue\n",
    "        \n",
    "        #    if ((tmpom - tmptm)**2/tmptm is not np.nan):\n",
    "        #        print (\"(tmpom - tmptm)**2/tmptm\",(tmpom - tmptm)**2/tmptm)\n",
    "        loss += (tmpom - tmptm)**2/tmptm\n",
    "    \n",
    "    \n",
    "    if (loss == 0.):\n",
    "        print(\"minx,maxx,binss,n_bins:\",minx,maxx,binss,n_bins)    \n",
    "        print(outputa,trutha)\n",
    "        for i in range(binss.size-1):\n",
    "            tmpo = outputa[ (trutha > binss[i]) &  (trutha < binss[i+1])  ]\n",
    "            tmpt = trutha[ (trutha > binss[i]) &  (trutha < binss[i+1])  ]\n",
    "            print(\"tmpo,tmpt:\",tmpo,tmpt)\n",
    "            if (list(tmpo.size())[0] == 0 or list(tmpt.size())[0] == 0):\n",
    "                continue\n",
    "            #print(list(tmpo.size())[0],list(tmpt.size())[0])\n",
    "            tmpom = torch.mean(tmpo)\n",
    "            tmptm = torch.mean(tmpt)\n",
    "            print (tmpom,tmptm)\n",
    "            loss += (tmpom - tmptm)**2/tmptm\n",
    "            print(loss)\n",
    "        \n",
    "    #print(\"loss: \",loss)\n",
    "    return loss        \n",
    "losscat = torch.nn.CrossEntropyLoss()    \n",
    "def categorical_loss(outputa,trutha,alpha):\n",
    "    total_loss =  losscat(outputa[:,:2],trutha[:,0].long()) + alpha*resoloss(outputa[:,2],trutha[:,1])\n",
    "    return total_loss\n",
    "    \n",
    "\n",
    "#model.train()\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    loss = []\n",
    "    for data in tqdm(trainloader):\n",
    "            data = data.to(device)        \n",
    "            optimizer.zero_grad()\n",
    "            result = model(data)\n",
    "            #lossc = resoloss(result, data.y, data)\n",
    "            #print(\"result:\",result)\n",
    "            #print(\"truth:\",data.z)\n",
    "            lossc = categorical_loss(result, data.z, 1.0)\n",
    "#            mse = F.mse_loss(result, data.y, reduction='mean')\n",
    "#            mse = criterion(result, data.y)\n",
    "#            print('result, y:',result,data.y)\n",
    "#            print('crit, orig:',criterion(result, data.y),F.mse_loss(result, data.y, reduction='mean'))\n",
    "            loss.append(lossc.item()) \n",
    "            lossc.backward()\n",
    "#            print(mse)\n",
    "            optimizer.step()\n",
    "            scheduler.batch_step()\n",
    "    print( 'batches for train:',len(loss)) \n",
    "#    print('loss',loss)\n",
    "    print('train loss:',np.mean(np.array(loss)))\n",
    "    del loss #memtest\n",
    "#    print(data)\n",
    "\n",
    "from scipy.stats import norm\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def gaussian(x,  mean,a, sigma):\n",
    "    return a * np.exp(-((x - mean)**2 / (2 * sigma**2)))\n",
    "\n",
    "def evaluate(epoch):\n",
    "        \"\"\"\"Evaluate the model\"\"\"\n",
    "        model.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "#        loss = []\n",
    "#        frac = []\n",
    "\n",
    "        pred = []\n",
    "        true = []\n",
    "        loss= []\n",
    "        \n",
    "        correct = 0\n",
    "        predc = []\n",
    "        truec = []\n",
    "        for data in tqdm(testloader):\n",
    "            data = data.to(device)        \n",
    "            result = model(data)\n",
    "            #lossc = resoloss(result, data.y)\n",
    "            lossc = categorical_loss(result, data.z, 1.0)\n",
    "#            print (result.item(),data.y.item())\n",
    "#            frac.append((result.item() - data.y.item())/data.y.item())\n",
    "            loss.append(lossc.item())\n",
    "\n",
    "            for i in result:\n",
    "                pred.append(i.detach().cpu()[2])\n",
    "                predc.append(i.detach().cpu()[0:2].argmax())\n",
    "            for i in data.z.detach():\n",
    "                true.append(i.detach().cpu()[1])\n",
    "                truec.append(i.detach().cpu()[0])\n",
    "            \n",
    "            \n",
    "        #print(predc,truec)\n",
    "        predc = np.array(predc)\n",
    "        truec = np.array(truec)\n",
    "        print(\"accuracy  :\",np.equal(predc,truec).sum()/len(truec))\n",
    "        \n",
    "        print('batches for test:', len(loss)) \n",
    "        print('test loss:',np.mean(np.array(loss)))\n",
    "#        fracarr = np.array(frac\n",
    "\n",
    "\n",
    "        preda = np.array(pred)\n",
    "        truea = np.array(true)\n",
    "        #preda = preda[:,2] ### added\n",
    "        #truea = truea[:,1] ### added\n",
    "        fracarr = (preda - truea)/truea\n",
    "        #print(preda,truea,fracarr)\n",
    "        print('pred - true / true mean:',(np.mean(fracarr)))\n",
    "        print('pred - true / true std:',(np.std(fracarr)))\n",
    "        (mu, sigma) = norm.fit(fracarr)\n",
    "        print('mu,sig:',mu,sigma)\n",
    "        \n",
    "\n",
    "        bin_heights, bin_borders, _ = plt.hist(fracarr,range=[-2,2], bins=100, label='histogram')\n",
    "        bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "        \n",
    "        try:\n",
    "            popt, _ = curve_fit(gaussian, bin_centers, bin_heights, p0=[0., 100., 1.],bounds = ([-np.inf,0,0],[np.inf,np.inf,np.inf]))\n",
    "            x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 100)\n",
    "            plt.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), label='fit')\n",
    "            plt.legend()\n",
    "\n",
    "\n",
    "            plt.xlabel('pred - true / true')\n",
    "            plt.ylabel('counts')\n",
    "            #plt.yscale(\"log\")\n",
    "            #plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "            plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(popt[0], popt[2]))\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        except RuntimeError:\n",
    "            print(\"Error - curve_fit failed\")\n",
    "            plt.xlabel('pred - true / true')\n",
    "            plt.ylabel('counts')\n",
    "            #plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "            plt.title('pred - true / true fit failed')\n",
    "            #plt.yscale(\"log\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "        return np.mean(np.array(loss))\n",
    "        del pred,true,loss,preda,truea,fracarr  #memtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "#checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_testtesttesttest_dyn2addlayer/'\n",
    "#checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_add1ip1dyn_binresoloss/'\n",
    "checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_phopi_comb_categ/'\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_loss = 99999999\n",
    "for epoch in range(1, 100):\n",
    "    print ('epoch:',epoch)\n",
    "    train(epoch)\n",
    "    loss_epoch = evaluate(epoch)\n",
    "    checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint_file = 'model_epoch_%03i.pth.tar' % ( epoch )\n",
    "    torch.save(checkpoint,\n",
    "                   os.path.join(checkpoint_dir,checkpoint_file ))\n",
    "    if loss_epoch < best_loss:\n",
    "        best_loss = loss_epoch\n",
    "        print('new best test loss:',best_loss)\n",
    "        torch.save(checkpoint,\n",
    "                   os.path.join(checkpoint_dir,'model_checkpoint_best.pth.tar' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for train continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fname = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_phopi_comb_categ/model_cont2_epoch_085.pth.tar'\n",
    "\n",
    "model=Net().to(device)\n",
    "#model = Net().to(device)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "model.load_state_dict(torch.load(model_fname)['state_dict'])\n",
    "#modl.eval()\n",
    "optimizer.load_state_dict(torch.load(model_fname)['optimizer'])\n",
    "\n",
    "epochlast = torch.load(model_fname)['epoch']\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_testtesttesttest_dyn2addlayer/'\n",
    "#checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_add1ip1dyn_binresoloss/'\n",
    "checkpoint_dir = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_phopi_comb_categ/'\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_loss = 99999999\n",
    "for epoch in range(int(epochlast), int(epochlast+100)):\n",
    "    print ('epoch:',epoch)\n",
    "    train(epoch)\n",
    "    loss_epoch = evaluate(epoch)\n",
    "    checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    checkpoint_file = 'model_cont3_epoch_%03i.pth.tar' % ( epoch )\n",
    "    torch.save(checkpoint,\n",
    "                   os.path.join(checkpoint_dir,checkpoint_file ))\n",
    "    if loss_epoch < best_loss:\n",
    "        best_loss = loss_epoch\n",
    "        print('new best test loss:',best_loss)\n",
    "        torch.save(checkpoint,\n",
    "                   os.path.join(checkpoint_dir,'model_cont3_checkpoint_best.pth.tar' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_fname = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_testtesttesttest_dyn2addlayer/model_checkpoint_best.pth.tar'\n",
    "#model_fname = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_add1ip1dyn_binresoloss/model_checkpoint_best.pth.tar'\n",
    "#model_fname = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_2dynoutputact/model_checkpoint_best.pth.tar'\n",
    "#model_fname = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_photon50k_scaleLOGSPLIT/model_checkpoint_best.pth.tar'\n",
    "model_fname = '/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/ouput_regression_phopi_comb_categ/model_cont3_checkpoint_best.pth.tar'\n",
    "\n",
    "\n",
    "mdl=Net().to(device)\n",
    "mdl.load_state_dict(torch.load(model_fname)['state_dict'])\n",
    "mdl.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "#mdl.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "#model.eval()\n",
    "pred = []\n",
    "true = []\n",
    "predc = []\n",
    "misp = []\n",
    "mist = []\n",
    "#testloader2 = torch_geometric.data.DataLoader(data_list[totalev-3000:totalev], batch_size=1)\n",
    "#testloader2 = torch_geometric.data.DataLoader(data_list, batch_size=50)\n",
    "#valloader = torch_geometric.data.DataLoader(data_list_pho[totalevpho-10000:totalevpho], batch_size=30)\n",
    "valloader = torch_geometric.data.DataLoader(data_list_pho, batch_size=60)\n",
    "  \n",
    "    \n",
    "#print(pred)\n",
    "#print(true)\n",
    "for data in tqdm(valloader):\n",
    "    data = data.to(device)        \n",
    "    result = mdl(data)\n",
    "    #print(data.y)\n",
    "    #print(result.i http://127.0.0.1:8005/?token=dcee9ad154badcbcaf8c1643ad620820eec6de751c69d92dtem().detach())\n",
    "    #pred.append(result.item())\n",
    "    #true.append(data.y.item())\n",
    "    for i,j in zip(result,data.z.detach()):\n",
    "        pred.append(i.detach().cpu()[2])\n",
    "        predc.append(i.detach().cpu()[0:2].argmax())\n",
    "        if (i.detach().cpu()[0:2].argmax() == 1):\n",
    "            misp.append(i.detach().cpu()[2])\n",
    "            mist.append(j.detach().cpu()[1])\n",
    "        #pred.append(i.item())\n",
    "#    print (pred) \n",
    "    #for i in data.z.detach():\n",
    "        true.append(j.detach().cpu()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred))\n",
    "print(len(true))\n",
    "print(np.unique(np.array(predc),return_counts=True)[1][0])\n",
    "print(\"pho acc, miss:\",np.unique(np.array(predc),return_counts=True)[1][0]/len(predc),np.unique(np.array(predc),return_counts=True)[1][1]/len(predc))\n",
    "#plt.hist(np.exp(misp),bins=50)\n",
    "plt.scatter(np.exp(mist),np.exp(misp),alpha=0.3)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotpred(axs,pred,true,e1,e2):\n",
    "#    preda = np.array(pred)\n",
    "#    truea = np.array(true)\n",
    "    fracarr = (pred - true)/true\n",
    "#    if e2<250 : \n",
    "#        bin_heights, bin_borders, _ = plt.hist(fracarr,range=[-2,2], bins=20, label='histogram')\n",
    "#    else:\n",
    "    bin_heights, bin_borders, _ = axs.hist(fracarr,range=[-1.0,1.0], bins=100, label='histogram')\n",
    "\n",
    "    bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "    popt, pcov = curve_fit(gaussian, bin_centers, bin_heights, p0=[0., 1000., 0.1],bounds = ([-np.inf,0,0],[np.inf,np.inf,np.inf]))\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    #print(popt,perr)\n",
    "\n",
    "    x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 100)\n",
    "    axs.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), label='fit')\n",
    "    axs.legend()\n",
    "\n",
    "\n",
    "    axs.set_xlabel('pred - true / true')\n",
    "    axs.set_ylabel('counts')\n",
    "    #plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "    axs.set_title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f,\\ E=$%i to %i' %(popt[0], popt[2],e1,e2))\n",
    "    axs.grid(True)\n",
    "    \n",
    "#    plt.show()\n",
    "    return [popt[0], popt[2],perr[0], perr[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins =  np.linspace(0,1000,21)\n",
    "#bins =  np.linspace(0,250,6)\n",
    "bins =  np.linspace(0,200,11)\n",
    "bins2 = np.linspace(250,1000,16)\n",
    "bins = np.append(bins,bins2)\n",
    "print(bins)\n",
    "\n",
    "pred = np.array(pred)\n",
    "true = np.array(true)\n",
    "\n",
    "print('before scaling pred',pred[0])\n",
    "print('before scaling true',true[0])\n",
    "\n",
    "pred  = np.exp(pred)\n",
    "true = np.exp(true)\n",
    "print('after scaling pred',pred[0])\n",
    "print('after scaling true',true[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seoe = []\n",
    "seoer = []\n",
    "me = []\n",
    "mer = []\n",
    "\n",
    "fig, axs = plt.subplots(6,5, figsize=(40, 40), facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in tqdm(range (bins.size - 1)):\n",
    "    preda = pred[(true >bins[i]) & (true <bins[i+1]) ]\n",
    "    truea = true[(true >bins[i]) & (true <bins[i+1]) ]\n",
    "    vals = plotpred(axs[i],preda,truea,bins[i],bins[i+1])\n",
    "    #print (vals)\n",
    "    #seoe.append(vals[1]/bins[i+1])\n",
    "    seoe.append(vals[1])\n",
    "    seoer.append(vals[3])\n",
    "    me.append(vals[0])\n",
    "    mer.append(vals[2])\n",
    "    #print (bins[i],bins[i+1])\n",
    "seoear = np.array(seoe)\n",
    "seoerar = np.array(seoer)\n",
    "mear = np.array(me)\n",
    "merar = np.array(mer)\n",
    "#print(seoear,'\\n',seoerar)\n",
    "fig.savefig('outputs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)\n",
    "print (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbinner = lambda t: t+10 if (t<200) else t+25\n",
    "vfunc = np.vectorize(plotbinner)\n",
    "newbin = vfunc(bins[:bins.size-1])\n",
    "#plt.plot(bins[:5]+50,(seoear)*100,yerr=seoerar)\n",
    "plt.errorbar(newbin,(seoear), yerr=seoerar, fmt='o', ecolor='orangered',color='steelblue', capsize=2,ls='--');\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('sigma')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(newbin,(mear), yerr=merar, fmt='o', ecolor='orangered',color='steelblue', capsize=2,ls='--');\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch.load(i)\n",
    "def reso(x,stoch,C):\n",
    "#    return np.sqrt(stoch**2/x + e_dep**2/x**2 + C**2 )\n",
    "    return np.sqrt(stoch**2/x  + C**2 )\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "popt, pcov = curve_fit(f=reso,xdata=newbin, ydata=seoear, sigma=seoerar, p0=[1.,1.],bounds = ([0,0],[np.inf,np.inf]))\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "print(popt,perr)\n",
    "\n",
    "x_interval_for_fit = np.linspace(5,1000, 100)\n",
    "plt.plot(x_interval_for_fit, reso(x_interval_for_fit, *popt), label='fit')\n",
    "plt.errorbar(newbin,(seoear), yerr=seoerar, fmt='o', ecolor='orangered',color='steelblue', capsize=2,ls='--');\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('sigma')\n",
    "#plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "plt.title('Resolution: stochastic=%.3f,  constant=%.3f'%(popt[0], popt[1]))\n",
    "print(popt,'\\n',pcov)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PION PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred,true,valloader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#mdl.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "#model.eval()\n",
    "pred = []\n",
    "true = []\n",
    "predc = []\n",
    "misp = []\n",
    "mist = []\n",
    "#testloader2 = torch_geometric.data.DataLoader(data_list[totalev-3000:totalev], batch_size=1)\n",
    "#testloader2 = torch_geometric.data.DataLoader(data_list, batch_size=50)\n",
    "#valloader = torch_geometric.data.DataLoader(data_list_pi[totalevpi-14000:totalevpi], batch_size=30)\n",
    "valloader = torch_geometric.data.DataLoader(data_list_pi, batch_size=60)\n",
    "\n",
    "\n",
    "\n",
    "for data in tqdm(valloader):\n",
    "    data = data.to(device)        \n",
    "    result = mdl(data)\n",
    "    #print(data.y)\n",
    "    #print(result.i http://127.0.0.1:8005/?token=dcee9ad154badcbcaf8c1643ad620820eec6de751c69d92dtem().detach())\n",
    "    #pred.append(result.item())\n",
    "    #true.append(data.y.item())\n",
    "    for i,j in zip(result,data.z.detach()):\n",
    "        pred.append(i.detach().cpu()[2])\n",
    "        predc.append(i.detach().cpu()[0:2].argmax())\n",
    "        if (i.detach().cpu()[0:2].argmax() == 0):\n",
    "            misp.append(i.detach().cpu()[2])\n",
    "            mist.append(j.detach().cpu()[1])\n",
    "        #pred.append(i.item())\n",
    "#    print (pred) \n",
    "    #for i in data.z.detach():\n",
    "        true.append(j.detach().cpu()[1])\n",
    "\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred))\n",
    "print(len(true))\n",
    "print(np.unique(np.array(predc),return_counts=True))\n",
    "print(\"pi acc, miss:\",np.unique(np.array(predc),return_counts=True)[1][1]/len(predc),np.unique(np.array(predc),return_counts=True)[1][0]/len(predc))\n",
    "#plt.hist(np.exp(mist),bins=50)\n",
    "plt.scatter(np.exp(mist),np.exp(misp),alpha=0.3)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotpred(axs,pred,true,e1,e2):\n",
    "#    preda = np.array(pred)\n",
    "#    truea = np.array(true)\n",
    "    fracarr = (pred - true)/true\n",
    "#    if e2<250 : \n",
    "#        bin_heights, bin_borders, _ = plt.hist(fracarr,range=[-2,2], bins=20, label='histogram')\n",
    "#    else:\n",
    "    bin_heights, bin_borders, _ = axs.hist(fracarr,range=[-1.0,1.0], bins=100, label='histogram')\n",
    "\n",
    "    bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "    popt, pcov = curve_fit(gaussian, bin_centers, bin_heights, p0=[0., 1000., 0.1],bounds = ([-np.inf,0,0],[np.inf,np.inf,np.inf]))\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    #print(popt,perr)\n",
    "\n",
    "    x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 100)\n",
    "    axs.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), label='fit')\n",
    "    axs.legend()\n",
    "\n",
    "\n",
    "    axs.set_xlabel('pred - true / true')\n",
    "    axs.set_ylabel('counts')\n",
    "    #plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "    axs.set_title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f,\\ E=$%i to %i' %(popt[0], popt[2],e1,e2))\n",
    "    axs.grid(True)\n",
    "    \n",
    "#    plt.show()\n",
    "    return [popt[0], popt[2],perr[0], perr[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins =  np.linspace(0,1000,21)\n",
    "#bins =  np.linspace(0,250,6)\n",
    "bins =  np.linspace(0,200,11)\n",
    "bins2 = np.linspace(250,1000,16)\n",
    "bins = np.append(bins,bins2)\n",
    "print(bins)\n",
    "\n",
    "pred = np.array(pred)\n",
    "true = np.array(true)\n",
    "\n",
    "print('before scaling pred',pred[0])\n",
    "print('before scaling true',true[0])\n",
    "\n",
    "pred  = np.exp(pred)\n",
    "true = np.exp(true)\n",
    "print('after scaling pred',pred[0])\n",
    "print('after scaling true',true[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seoe = []\n",
    "seoer = []\n",
    "me = []\n",
    "mer = []\n",
    "\n",
    "fig, axs = plt.subplots(6,5, figsize=(40, 40), facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in tqdm(range (bins.size - 1)):\n",
    "    preda = pred[(true >bins[i]) & (true <bins[i+1]) ]\n",
    "    truea = true[(true >bins[i]) & (true <bins[i+1]) ]\n",
    "    vals = plotpred(axs[i],preda,truea,bins[i],bins[i+1])\n",
    "    #print (vals)\n",
    "    #seoe.append(vals[1]/bins[i+1])\n",
    "    seoe.append(vals[1])\n",
    "    seoer.append(vals[3])\n",
    "    me.append(vals[0])\n",
    "    mer.append(vals[2])\n",
    "    #print (bins[i],bins[i+1])\n",
    "seoear = np.array(seoe)\n",
    "seoerar = np.array(seoer)\n",
    "mear = np.array(me)\n",
    "merar = np.array(mer)\n",
    "#print(seoear,'\\n',seoerar)\n",
    "fig.savefig('outputs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbinner = lambda t: t+10 if (t<200) else t+25\n",
    "vfunc = np.vectorize(plotbinner)\n",
    "newbin = vfunc(bins[:bins.size-1])\n",
    "#plt.plot(bins[:5]+50,(seoear)*100,yerr=seoerar)\n",
    "plt.errorbar(newbin,(seoear), yerr=seoerar, fmt='o', ecolor='orangered',color='steelblue', capsize=2,ls='--');\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('sigma')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(newbin,(mear), yerr=merar, fmt='o', ecolor='orangered',color='steelblue', capsize=2,ls='--');\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.load(i)\n",
    "def reso(x,stoch,C):\n",
    "#    return np.sqrt(stoch**2/x + e_dep**2/x**2 + C**2 )\n",
    "    return np.sqrt(stoch**2/x  + C**2 )\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "popt, pcov = curve_fit(f=reso,xdata=newbin, ydata=seoear, sigma=seoerar, p0=[1.,1.],bounds = ([0,0],[np.inf,np.inf]))\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "print(popt,perr)\n",
    "\n",
    "x_interval_for_fit = np.linspace(5,1000, 100)\n",
    "plt.plot(x_interval_for_fit, reso(x_interval_for_fit, *popt), label='fit')\n",
    "plt.errorbar(newbin,(seoear), yerr=seoerar, fmt='o', ecolor='orangered',color='steelblue', capsize=2,ls='--');\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('sigma')\n",
    "#plt.title(r'$\\mathrm{pred - true / true:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "plt.title('Resolution: stochastic=%.3f,  constant=%.3f'%(popt[0], popt[1]))\n",
    "print(popt,'\\n',pcov)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
