{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from datasets.graph import draw_sample\n",
    "import torch\n",
    "\n",
    "model_fname = '/home/lagray/hgcal_ldrd/checkpoints/model_checkpoint_EdgeNetWithCategories_264403_5b5c05404f_lagray.best.pth.tar'\n",
    "\n",
    "test_fname = '/home/lagray/training_data/single_taus/test/processed/data_40.pt'\n",
    "\n",
    "#weird stuff\n",
    "#'/home/lagray/training_data/single_taus/test/processed/data_39.pt'\n",
    "#'/home/lagray/training_data/single_taus/test/processed/data_41.pt'\n",
    "#'/home/lagray/training_data/single_taus/test/processed/data_2.pt'\n",
    "#'/home/lagray/training_data/single_taus/test/processed/data_26.pt'\n",
    "\n",
    "from models.EdgeNetWithCategories import EdgeNetWithCategories\n",
    "\n",
    "mdl = EdgeNetWithCategories(input_dim=5, hidden_dim=64, output_dim=4, n_iters=6).to('cuda:0')\n",
    "\n",
    "mdl.load_state_dict(torch.load(model_fname)['model'])\n",
    "mdl.eval()\n",
    "\n",
    "test_fname = '/home/lagray/training_data/single_taus/processed/data_8069.pt'\n",
    "\n",
    "data = torch.load(test_fname).to('cuda:0')\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_edges = mdl(data).detach()\n",
    "    pred_edges_np = pred_edges.cpu().numpy()\n",
    "    \n",
    "print(np.unique(np.argmax(pred_edges_np,axis=-1), return_counts=True))\n",
    "print(torch.unique(data.y.cpu(), return_counts=True))\n",
    "\n",
    "X = data.x.cpu().numpy()\n",
    "index = data.edge_index.cpu().numpy().T\n",
    "Ro = index[:,0]\n",
    "Ri = index[:,1]\n",
    "y = data.y.cpu().numpy()\n",
    "\n",
    "out = np.argmax(pred_edges_np,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unionfind import UnionFind\n",
    "finder_had = UnionFind(X.shape[0])\n",
    "finder_pho = UnionFind(X.shape[0])\n",
    "finder_mip = UnionFind(X.shape[0])\n",
    "\n",
    "for i in tqdm(range(index.shape[0])):\n",
    "    if out[i] == 1:\n",
    "        finder_had.union(index[i,0], index[i,1])\n",
    "    if out[i] == 2:\n",
    "        finder_pho.union(index[i,0], index[i,1])\n",
    "    if out[i] == 3:\n",
    "        finder_mip.union(index[i,0], index[i,1])\n",
    "\n",
    "had_roots = np.array([finder_had.find(i) for i in range(X.shape[0])], dtype=np.uint32)\n",
    "pho_roots = np.array([finder_pho.find(i) for i in range(X.shape[0])], dtype=np.uint32)\n",
    "mip_roots = np.array([finder_mip.find(i) for i in range(X.shape[0])], dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "had_clusters = np.unique(had_roots, return_inverse=True, return_counts=True)\n",
    "pho_clusters = np.unique(pho_roots, return_inverse=True, return_counts=True)\n",
    "mip_clusters = np.unique(mip_roots, return_inverse=True, return_counts=True)\n",
    "\n",
    "hads = had_clusters[0][np.where(had_clusters[2] > 4)]\n",
    "ems = pho_clusters[0][np.where(pho_clusters[2] > 4)]\n",
    "mips = mip_clusters[0][np.where(mip_clusters[2] > 4)]\n",
    "\n",
    "had_clusters_sel = {i: np.where(had_roots == had)[0] for i, had in enumerate(hads)}\n",
    "em_clusters_sel = {i: np.where(pho_roots == em)[0] for i, em in enumerate(ems)}\n",
    "mip_clusters_sel = {i: np.where(mip_roots == mip)[0] for i, mip in enumerate(mips)}\n",
    "\n",
    "print(had_clusters_sel)\n",
    "print(em_clusters_sel)\n",
    "print(mip_clusters_sel)\n",
    "\n",
    "print(had_clusters[2][np.where(had_clusters[2] > 4)])\n",
    "print(pho_clusters[2][np.where(pho_clusters[2] > 4)])\n",
    "print(mip_clusters[2][np.where(mip_clusters[2] > 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "from torch_geometric.nn import EdgeConv\n",
    "from torch_geometric.nn.pool.edge_pool import EdgePooling\n",
    "\n",
    "from models.DynamicReductionNetwork import DynamicReductionNetwork\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "print(type(data))\n",
    "\n",
    "def print_model_summary(model):\n",
    "        \"\"\"Override as needed\"\"\"\n",
    "        print(\n",
    "            'Model: \\n%s\\nParameters: %i' %\n",
    "            (model, sum(p.numel() for p in model.parameters()))\n",
    "        )\n",
    "\n",
    "cls = []\n",
    "cls.extend([torch_geometric.data.Data(x = torch.tensor(X[clus]), y=torch.tensor([20. + np.random.exponential()])) for clus in had_clusters_sel.values()])\n",
    "cls.extend([torch_geometric.data.Data(x = torch.tensor(X[clus]), y=torch.tensor([.5 + np.random.exponential()])) for clus in em_clusters_sel.values()])\n",
    "cls.extend([torch_geometric.data.Data(x = torch.tensor(X[clus]), y=torch.tensor([1.])) for clus in mip_clusters_sel.values()])\n",
    "\n",
    "target = torch.tensor([30., 5., 2., 1.]).to('cpu')\n",
    "datas = torch_geometric.data.DataLoader(cls, batch_size = 60)\n",
    "\n",
    "#print(cl.x.shape, cl.x)\n",
    "\n",
    "test = DynamicReductionNetwork(input_dim=5, hidden_dim=64, output_dim=1, k=8, aggr='max').to('cuda:0')\n",
    "optimizer = torch.optim.Adam(test.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "\n",
    "print(print_model_summary(test))\n",
    "\n",
    "test.train()\n",
    "\n",
    "for e in tqdm(range(300)):\n",
    "    test.train()\n",
    "    optimizer.zero_grad()\n",
    "    for i, datum in enumerate(datas):\n",
    "        temp = datum.to('cuda:0')\n",
    "        result = test(temp, temp.batch) \n",
    "        mse = F.mse_loss(result, temp.y, reduction='mean')\n",
    "        mse.backward()\n",
    "        optimizer.step()\n",
    "        print(mse.item(), result, datum.y)\n",
    "\n",
    "#batch = torch.zeros(cl.x.shape[0], dtype=torch.int64).to('cuda:0')\n",
    "#val = test(cl, batch)\n",
    "#test(cl, batch).backward()\n",
    "#print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.exponential(size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from models.DynamicReductionNetwork import DynamicReductionNetwork\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "path = osp.join('./', '..', 'data', 'MNIST')\n",
    "transform = T.Cartesian(cat=False)\n",
    "train_dataset = MNISTSuperpixels(path, True, transform=transform)\n",
    "test_dataset = MNISTSuperpixels(path, False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "d = train_dataset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "\n",
    "i = 0\n",
    "mask = (d[i].x > 0).squeeze()\n",
    "plt.scatter(d[i].pos[mask,1], d[i].pos[mask,0])\n",
    "print(d[i].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('features ->', d.num_features)\n",
    "print('classes ->',d.num_classes)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drn = DynamicReductionNetwork(input_dim=3, hidden_dim=64,\n",
    "                                           k = 16,\n",
    "                                           output_dim=d.num_classes, aggr='add',\n",
    "                                           norm=torch.tensor([1., 1./27., 1./27.]))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        logits = self.drn(data)\n",
    "        return F.log_softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 16:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.005\n",
    "    \n",
    "    if epoch == 32:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "\n",
    "    if epoch == 48:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.00001\n",
    "\n",
    "    for data in tqdm(train_loader):\n",
    "        data = data.to(device)        \n",
    "        mask = (data.x > 0.).squeeze()\n",
    "        data.x = torch.cat([data.x, data.pos], dim=-1)\n",
    "        data.x = data.x[mask,:]\n",
    "        #print(data.x)\n",
    "        data.pos = data.pos[mask,:]\n",
    "        data.batch = data.batch[mask.squeeze()]\n",
    "        optimizer.zero_grad()\n",
    "        result = model(data)\n",
    "        F.nll_loss(result, data.y).backward()\n",
    "        #print(torch.unique(torch.argmax(result, dim=-1)))\n",
    "        #print(torch.unique(data.y))\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        mask = (data.x > 0.).squeeze()\n",
    "        data.x = torch.cat([data.x, data.pos], dim=-1)\n",
    "        data.x = data.x[mask,:]\n",
    "        #print(data.x)\n",
    "        data.pos = data.pos[mask,:]\n",
    "        data.batch = data.batch[mask.squeeze()]\n",
    "        pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(test_dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 65):\n",
    "    train(epoch)\n",
    "    test_acc = test()\n",
    "    print('Epoch: {:02d}, Test: {:.4f}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
