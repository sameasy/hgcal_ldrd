{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import normalized_cut\n",
    "from torch_geometric.nn import (NNConv, graclus, max_pool, max_pool_x,\n",
    "                                global_mean_pool)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "from datasets.hitgraphs import HitGraphDataset\n",
    "\n",
    "import tqdm\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed = False\n",
    "sig_weight = 1.0\n",
    "bkg_weight = 1.0\n",
    "train_batch_size = 1\n",
    "valid_batch_size = 1\n",
    "n_epochs = 20\n",
    "lr = 0.01\n",
    "hidden_dim = 64\n",
    "n_iters = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    }
   ],
   "source": [
    "from training.gnn import GNNTrainer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device %s'%device)\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sameasy2006/npztau_k8/tau_hfntup_0to1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/19200 [00:00<42:35,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19200/19200 [37:51<00:00,  8.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "19200 [15360 15360 19200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#path = osp.join(os.environ['GNN_TRAINING_DATA_ROOT'], args.dataset)\n",
    "path ='/home/sameasy2006/npztau_k8/tau_hfntup_0to1000'\n",
    "print(path)\n",
    "full_dataset = HitGraphDataset(path, directed=directed, categorical=False)\n",
    "fulllen = len(full_dataset)\n",
    "tv_frac = 0.20\n",
    "tv_num = math.ceil(fulllen*tv_frac)\n",
    "splits = np.cumsum([fulllen-tv_num,0,tv_num])\n",
    "print(fulllen, splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-04 21:32:47,789 - GNNTrainer - INFO - Model: \n",
      "EdgeNet2(\n",
      "  (inputnet): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "  )\n",
      "  (edgenetwork): Sequential(\n",
      "    (0): Linear(in_features=778, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (nodenetwork): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=138, out_features=101, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=101, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      ")\n",
      "Parameters: 21858\n",
      "2020-02-04 21:32:47,790 - GNNTrainer - INFO - Epoch 0\n",
      "loss = 0.01205: 100%|██████████| 15360/15360 [12:12<00:00, 20.97it/s]\n",
      "2020-02-04 21:45:00,166 - GNNTrainer - DEBUG -  Processed 15360 batches\n",
      "2020-02-04 21:45:00,168 - GNNTrainer - INFO -   Training loss: 0.06800\n",
      "2020-02-04 21:45:00,169 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "100%|██████████| 3840/3840 [01:43<00:00, 36.93it/s]\n",
      "2020-02-04 21:46:44,199 - GNNTrainer - DEBUG -  Processed 3840 samples in 3840 batches\n",
      "2020-02-04 21:46:44,201 - GNNTrainer - INFO -   Validation loss: 0.015 acc: 0.994\n",
      "2020-02-04 21:46:44,201 - GNNTrainer - INFO -   signal_efficiency: 0.951157 background_efficiency: 0.994928\n",
      "2020-02-04 21:46:44,202 - GNNTrainer - INFO -   signal_purity: 0.773966 background_purity: 0.999104\n",
      "2020-02-04 21:46:44,202 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.01500\n",
      "2020-02-04 21:46:44,224 - GNNTrainer - INFO - Epoch 1\n",
      "loss = 0.01010: 100%|██████████| 15360/15360 [11:56<00:00, 21.42it/s]\n",
      "2020-02-04 21:58:41,179 - GNNTrainer - DEBUG -  Processed 15360 batches\n",
      "2020-02-04 21:58:41,180 - GNNTrainer - INFO -   Training loss: 0.01182\n",
      "2020-02-04 21:58:41,181 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "100%|██████████| 3840/3840 [01:41<00:00, 37.90it/s]\n",
      "2020-02-04 22:00:22,527 - GNNTrainer - DEBUG -  Processed 3840 samples in 3840 batches\n",
      "2020-02-04 22:00:22,528 - GNNTrainer - INFO -   Validation loss: 0.011 acc: 0.996\n",
      "2020-02-04 22:00:22,528 - GNNTrainer - INFO -   signal_efficiency: 0.920407 background_efficiency: 0.997238\n",
      "2020-02-04 22:00:22,529 - GNNTrainer - INFO -   signal_purity: 0.858845 background_purity: 0.998545\n",
      "2020-02-04 22:00:22,530 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.01078\n",
      "2020-02-04 22:00:22,536 - GNNTrainer - INFO - Epoch 2\n",
      "loss = 0.00437: 100%|██████████| 15360/15360 [11:56<00:00, 21.44it/s]\n",
      "2020-02-04 22:12:19,110 - GNNTrainer - DEBUG -  Processed 15360 batches\n",
      "2020-02-04 22:12:19,110 - GNNTrainer - INFO -   Training loss: 0.00888\n",
      "2020-02-04 22:12:19,111 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "100%|██████████| 3840/3840 [01:41<00:00, 37.72it/s]\n",
      "2020-02-04 22:14:00,946 - GNNTrainer - DEBUG -  Processed 3840 samples in 3840 batches\n",
      "2020-02-04 22:14:00,947 - GNNTrainer - INFO -   Validation loss: 0.005 acc: 0.998\n",
      "2020-02-04 22:14:00,947 - GNNTrainer - INFO -   signal_efficiency: 0.938152 background_efficiency: 0.999286\n",
      "2020-02-04 22:14:00,948 - GNNTrainer - INFO -   signal_purity: 0.959982 background_purity: 0.998871\n",
      "2020-02-04 22:14:00,948 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.00539\n",
      "2020-02-04 22:14:00,954 - GNNTrainer - INFO - Epoch 3\n",
      "loss = 0.00353: 100%|██████████| 15360/15360 [11:58<00:00, 21.37it/s]\n",
      "2020-02-04 22:25:59,826 - GNNTrainer - DEBUG -  Processed 15360 batches\n",
      "2020-02-04 22:25:59,827 - GNNTrainer - INFO -   Training loss: 0.00540\n",
      "2020-02-04 22:25:59,827 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "100%|██████████| 3840/3840 [01:42<00:00, 37.33it/s]\n",
      "2020-02-04 22:27:42,731 - GNNTrainer - DEBUG -  Processed 3840 samples in 3840 batches\n",
      "2020-02-04 22:27:42,732 - GNNTrainer - INFO -   Validation loss: 0.005 acc: 0.998\n",
      "2020-02-04 22:27:42,732 - GNNTrainer - INFO -   signal_efficiency: 0.932284 background_efficiency: 0.999518\n",
      "2020-02-04 22:27:42,733 - GNNTrainer - INFO -   signal_purity: 0.972470 background_purity: 0.998764\n",
      "2020-02-04 22:27:42,734 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.00500\n",
      "2020-02-04 22:27:42,739 - GNNTrainer - INFO - Epoch 4\n",
      "loss = 0.00319: 100%|██████████| 15360/15360 [11:54<00:00, 21.49it/s]\n",
      "2020-02-04 22:39:37,575 - GNNTrainer - DEBUG -  Processed 15360 batches\n",
      "2020-02-04 22:39:37,576 - GNNTrainer - INFO -   Training loss: 0.00528\n",
      "2020-02-04 22:39:37,576 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "100%|██████████| 3840/3840 [01:42<00:00, 37.51it/s]\n",
      "2020-02-04 22:41:19,986 - GNNTrainer - DEBUG -  Processed 3840 samples in 3840 batches\n",
      "2020-02-04 22:41:19,987 - GNNTrainer - INFO -   Validation loss: 0.004 acc: 0.999\n",
      "2020-02-04 22:41:19,987 - GNNTrainer - INFO -   signal_efficiency: 0.946565 background_efficiency: 0.999522\n",
      "2020-02-04 22:41:19,988 - GNNTrainer - INFO -   signal_purity: 0.973113 background_purity: 0.999025\n",
      "2020-02-04 22:41:19,989 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.00438\n",
      "2020-02-04 22:41:19,993 - GNNTrainer - INFO - Epoch 5\n",
      "loss = 0.00289: 100%|██████████| 15360/15360 [11:56<00:00, 21.44it/s]\n",
      "2020-02-04 22:53:16,490 - GNNTrainer - DEBUG -  Processed 15360 batches\n",
      "2020-02-04 22:53:16,490 - GNNTrainer - INFO -   Training loss: 0.00457\n",
      "2020-02-04 22:53:16,491 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "100%|██████████| 3840/3840 [01:42<00:00, 37.60it/s]\n",
      "2020-02-04 22:54:58,648 - GNNTrainer - DEBUG -  Processed 3840 samples in 3840 batches\n",
      "2020-02-04 22:54:58,649 - GNNTrainer - INFO -   Validation loss: 0.004 acc: 0.999\n",
      "2020-02-04 22:54:58,649 - GNNTrainer - INFO -   signal_efficiency: 0.959809 background_efficiency: 0.999475\n",
      "2020-02-04 22:54:58,650 - GNNTrainer - INFO -   signal_purity: 0.970902 background_purity: 0.999266\n",
      "2020-02-04 22:54:58,651 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.00398\n",
      "2020-02-04 22:54:58,656 - GNNTrainer - INFO - Epoch 6\n",
      "loss = 0.00267: 100%|██████████| 15360/15360 [11:56<00:00, 21.44it/s]\n",
      "2020-02-04 23:06:54,975 - GNNTrainer - DEBUG -  Processed 15360 batches\n",
      "2020-02-04 23:06:54,976 - GNNTrainer - INFO -   Training loss: 0.00442\n",
      "2020-02-04 23:06:54,977 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "100%|██████████| 3840/3840 [01:43<00:00, 37.23it/s]\n",
      "2020-02-04 23:08:38,157 - GNNTrainer - DEBUG -  Processed 3840 samples in 3840 batches\n",
      "2020-02-04 23:08:38,158 - GNNTrainer - INFO -   Validation loss: 0.004 acc: 0.999\n",
      "2020-02-04 23:08:38,159 - GNNTrainer - INFO -   signal_efficiency: 0.956225 background_efficiency: 0.999475\n",
      "2020-02-04 23:08:38,159 - GNNTrainer - INFO -   signal_purity: 0.970795 background_purity: 0.999201\n",
      "2020-02-04 23:08:38,163 - GNNTrainer - INFO - Epoch 7\n",
      "loss = 0.00358:  62%|██████▏   | 9485/15360 [07:19<04:50, 20.19it/s]"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=0,stop=splits[0]))\n",
    "#train_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=0,stop=5))\n",
    "valid_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=splits[1],stop=splits[2]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "train_samples = len(train_dataset)\n",
    "valid_samples = len(valid_dataset)\n",
    "\n",
    "d = full_dataset\n",
    "num_features = d.num_features\n",
    "num_classes = d[0].y.dim() if d[0].y.dim() == 1 else d[0].y.size(1)\n",
    "\n",
    "#if args.categorized:\n",
    "#    if not args.forcecats:\n",
    "#       num_classes = int(d[0].y.max().item()) + 1 if d[0].y.dim() == 1 else d[0].y.size(1)\n",
    "#    else:\n",
    "#        num_classes = args.cats\n",
    "\n",
    "print ('num_classes',num_classes)\n",
    "#the_weights = np.array([1., 1., 1., 1.]) #[0.017, 1., 1., 10.]\n",
    "the_weights = np.array([1,1]) #[0.017, 1., 1., 10.]\n",
    "trainer = GNNTrainer(category_weights = the_weights, \n",
    "                     output_dir='/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/output_tau8nn', device=device)\n",
    "\n",
    "trainer.logger.setLevel(logging.DEBUG)\n",
    "strmH = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "strmH.setFormatter(formatter)\n",
    "trainer.logger.addHandler(strmH)\n",
    "\n",
    "#example lr scheduling definition\n",
    "def lr_scaling(optimizer):\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau        \n",
    "    return ReduceLROnPlateau(optimizer, mode='min', verbose=True,\n",
    "                             min_lr=5e-7, factor=0.2, \n",
    "                             threshold=0.01, patience=5)\n",
    "\n",
    "\n",
    "trainer.build_model(name='EdgeNet2', loss_func='binary_cross_entropy',\n",
    "                    optimizer='Adam', learning_rate=0.001, lr_scaling=lr_scaling,\n",
    "                    input_dim=num_features, hidden_dim=64, n_iters=6,\n",
    "                    output_dim=num_classes)\n",
    "\n",
    "trainer.print_model_summary()\n",
    "\n",
    "train_summary = trainer.train(train_loader, n_epochs, valid_data_loader=valid_loader)\n",
    "\n",
    "print(train_summary)\n",
    "    \n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--categorized', '-c', action='store_true', default=False, help='Does the model you want to train have explicit categories?')\n",
    "    parser.add_argument('--forcecats', action='store_true', default=False, help='Do we want to force the number of categories?')\n",
    "    parser.add_argument('--cats', default=1, type=int, help='Number of categories to force')\n",
    "    parser.add_argument('--optimizer', '-o', default='Adam', help='Optimizer to use for training.')\n",
    "    parser.add_argument('--model', '-m', default='EdgeNet2', help='The model to train.')\n",
    "    parser.add_argument('--loss', '-l', default='binary_cross_entropy', help='Loss function to use in training.')\n",
    "    parser.add_argument('--lr', default=0.001, type=float, help='The starting learning rate.')\n",
    "    parser.add_argument('--hidden_dim', default=64, type=int, help='Latent space size.')\n",
    "    parser.add_argument('--n_iters', default=6, type=int, help='Number of times to iterate the graph.')\n",
    "    parser.add_argument('--dataset', '-d', default='single_photon')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "'''                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = osp.join(os.environ['GNN_TRAINING_DATA_ROOT'], 'photon_hfntup_0to1000')\n",
    "#print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= [0, 1, 2, 3, 4, 5, 6]\n",
    "lrs= [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001] \n",
    "train_times= [225.54388546943665, 225.22212648391724, 225.3216323852539, 225.35716199874878, 225.40882205963135, 225.19546961784363, 225.20603489875793] \n",
    "train_losss= [0.04032796977131486, 0.015438518584319796, 0.012363892024644151, 0.008919628991402343, 0.0075437994571782635, 0.00695315887611514, 0.0066568848183646955] \n",
    "valid_times= [19.107330799102783, 19.0887508392334, 19.067117929458618, 19.119823217391968, 19.130934476852417, 19.135619401931763, 19.068397998809814]\n",
    "valid_losss= [0.017775222663031956, 0.014626435511804124, 0.010906822071867673, 0.007683055135360127, 0.006854731867877541, 0.006997309575743, 0.007357149830413887]\n",
    "valid_accs=[0.9932499189329411, 0.9944335828067422, 0.9956929219621333, 0.9973105070697365, 0.9977536107685164, 0.997650872746129, 0.9975113442820626]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(epochs,train_losss,label='training loss')\n",
    "plt.plot(epochs,valid_losss,label='validation loss')\n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,valid_accs,label='valid  acc')\n",
    "#plt.plot(epochs,valid_losss,label='validation loss')\n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([0.7523, 0.7523, 0.7496])\n",
    "b=torch.tensor([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ((a<0)==(b<0))\n",
    "c.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the_weights = np.array([0.017,1])\n",
    "\n",
    "2020-02-02 09:15:52,559 - GNNTrainer - INFO - Epoch 10\n",
    "loss = 0.00050: 100%|██████████| 11520/11520 [04:59<00:00, 38.45it/s]\n",
    "2020-02-02 09:20:52,147 - GNNTrainer - DEBUG -  Processed 11520 batches\n",
    "2020-02-02 09:20:52,148 - GNNTrainer - INFO -   Training loss: 0.00052\n",
    "2020-02-02 09:20:52,148 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
    "100%|██████████| 2880/2880 [00:28<00:00, 100.21it/s]\n",
    "2020-02-02 09:21:20,912 - GNNTrainer - DEBUG -  Processed 2880 samples in 2880 batches\n",
    "2020-02-02 09:21:20,912 - GNNTrainer - INFO -   Validation loss: 0.020 acc: 0.993\n",
    "2020-02-02 09:21:20,913 - GNNTrainer - INFO -   signal_efficiency: 0.998849 background_efficiency: 0.993102\n",
    "2020-02-02 09:21:20,913 - GNNTrainer - INFO -   signal_purity: 0.821057 background_purity: 0.999963\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "2020-02-02 10:11:51,495 - GNNTrainer - INFO - Epoch 8\n",
    "loss = 0.00047: 100%|██████████| 11520/11520 [04:59<00:00, 38.45it/s]\n",
    "2020-02-02 10:16:51,110 - GNNTrainer - DEBUG -  Processed 11520 batches\n",
    "2020-02-02 10:16:51,110 - GNNTrainer - INFO -   Training loss: 0.00052\n",
    "2020-02-02 10:16:51,111 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
    "100%|██████████| 2880/2880 [00:28<00:00, 100.22it/s]\n",
    "2020-02-02 10:17:19,872 - GNNTrainer - DEBUG -  Processed 2880 samples in 2880 batches\n",
    "2020-02-02 10:17:19,873 - GNNTrainer - INFO -   Validation loss: 0.019 acc: 0.994\n",
    "2020-02-02 10:17:19,874 - GNNTrainer - INFO -   signal_efficiency: 0.998442 background_efficiency: 0.993346\n",
    "2020-02-02 10:17:19,874 - GNNTrainer - INFO -   signal_purity: 0.826232 background_purity: 0.999950\n",
    "2020-02-02 10:17:19,875 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.01923\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
