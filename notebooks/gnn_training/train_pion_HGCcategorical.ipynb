{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import normalized_cut\n",
    "from torch_geometric.nn import (NNConv, graclus, max_pool, max_pool_x,\n",
    "                                global_mean_pool)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "from datasets.hitgraphs import HitGraphDataset\n",
    "\n",
    "import tqdm\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed = False\n",
    "sig_weight = 1.0\n",
    "bkg_weight = 1.0\n",
    "train_batch_size = 1\n",
    "valid_batch_size = 1\n",
    "n_epochs = 20\n",
    "lr = 0.01\n",
    "hidden_dim = 64\n",
    "n_iters = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    }
   ],
   "source": [
    "from training.gnn2 import GNNTrainer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device %s'%device)\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sameasy2006/npz_hgcal_k8/pion_hgctup_0to1000\n",
      "20000 [16000 16000 20000]\n"
     ]
    }
   ],
   "source": [
    "#path = osp.join(os.environ['GNN_TRAINING_DATA_ROOT'], args.dataset)\n",
    "path ='/home/sameasy2006/npz_hgcal_k8/pion_hgctup_0to1000'\n",
    "print(path)\n",
    "full_dataset = HitGraphDataset(path, directed=directed, categorical=True)\n",
    "fulllen = len(full_dataset)\n",
    "tv_frac = 0.20\n",
    "tv_num = math.ceil(fulllen*tv_frac)\n",
    "splits = np.cumsum([fulllen-tv_num,0,tv_num])\n",
    "print(fulllen, splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-07 09:44:57,368 - GNNTrainer - INFO - Model: \n",
      "EdgeNetWithCategories(\n",
      "  (inputnet): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (edgenetwork): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
      "    (5): LogSoftmax()\n",
      "  )\n",
      "  (nodenetwork0): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=138, out_features=101, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=101, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      "  (nodenetwork1): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=138, out_features=101, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=101, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      "  (nodenetwork2): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=138, out_features=101, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=101, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      "  (nodenetwork3): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=138, out_features=101, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=101, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      "  (nodenetwork4): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=138, out_features=101, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=101, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      "  (nodenetwork5): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=138, out_features=101, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=101, out_features=64, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      ")\n",
      "Parameters: 264403\n",
      "2020-02-07 09:44:57,369 - GNNTrainer - INFO - Epoch 0\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_target: tensor([0, 0, 2,  ..., 0, 0, 0], device='cuda:0') batch_output: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0102, -1.3455, -1.3111, -2.2450],\n",
      "        [-1.2769, -1.1415, -1.1943, -2.3142],\n",
      "        [-0.9779, -1.3872, -1.3054, -2.2725],\n",
      "        ...,\n",
      "        [-1.0852, -1.1511, -1.4979, -2.1014],\n",
      "        [-1.1495, -1.3015, -1.4199, -1.7757],\n",
      "        [-1.1743, -1.0976, -1.5490, -1.9323]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "batch_target: tensor([1, 1, 1,  ..., 1, 1, 0], device='cuda:0') batch_output: tensor([[-1.1984, -1.4082, -1.1434, -2.0025],\n",
      "        [-1.2006, -1.3177, -1.2948, -1.8497],\n",
      "        [-1.2683, -1.4124, -1.1663, -1.8103],\n",
      "        ...,\n",
      "        [-0.9981, -1.3373, -1.4160, -2.0700],\n",
      "        [-1.0846, -1.3892, -1.3566, -1.8634],\n",
      "        [-1.1143, -1.4520, -1.2181, -1.9523]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "batch_target: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0') batch_output: tensor([[-1.4441, -1.2668, -1.1014, -1.8977],\n",
      "        [-1.1806, -1.1727, -1.2798, -2.2511],\n",
      "        [-1.2684, -1.4756, -1.0671, -1.9236],\n",
      "        ...,\n",
      "        [-1.2790, -1.3271, -1.2538, -1.7659],\n",
      "        [-1.2664, -1.4605, -1.1452, -1.7844],\n",
      "        [-1.2549, -1.3640, -1.2602, -1.7392]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "batch_target: tensor([0, 1, 1,  ..., 0, 0, 0], device='cuda:0') batch_output: tensor([[-1.5480, -1.3310, -0.9431, -2.0122],\n",
      "        [-1.5714, -1.3456, -0.8948, -2.0943],\n",
      "        [-1.6436, -1.3192, -0.9173, -1.9679],\n",
      "        ...,\n",
      "        [-1.1150, -1.3243, -1.4853, -1.7168],\n",
      "        [-1.1180, -1.3895, -1.4499, -1.6647],\n",
      "        [-1.1148, -1.4492, -1.2362, -1.9188]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss = 0.74330: 100%|██████████| 5/5 [00:01<00:00,  4.19it/s]\n",
      "2020-02-07 09:44:58,566 - GNNTrainer - DEBUG -  Processed 5 batches\n",
      "2020-02-07 09:44:58,568 - GNNTrainer - INFO -   Training loss: 1.48659\n",
      "2020-02-07 09:44:58,569 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_target: tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0') batch_output: tensor([[-3.2104, -1.4271, -0.4378, -2.6011],\n",
      "        [-3.5245, -1.3330, -0.5160, -2.2076],\n",
      "        [-3.2989, -1.4107, -0.4879, -2.2517],\n",
      "        ...,\n",
      "        [-1.2476, -1.0757, -1.2819, -2.3621],\n",
      "        [-1.2927, -1.0798, -1.1919, -2.4990],\n",
      "        [-1.0549, -1.2300, -1.2946, -2.4597]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 173/4000 [00:05<02:10, 29.37it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-40bc480c4ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_model_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtrain_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgcal_ldrd-gravnet2_wip_trainer_args/src/training/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data_loader, n_epochs, valid_data_loader)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0msum_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_data_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0msum_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgnn/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgcal_ldrd-gravnet2_wip_trainer_args/src/training/gnn2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;31m# Count number of correct predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m#print(batch_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=0,stop=splits[0]))\n",
    "train_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=0,stop=5))\n",
    "valid_dataset = torch.utils.data.Subset(full_dataset,np.arange(start=splits[1],stop=splits[2]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "train_samples = len(train_dataset)\n",
    "valid_samples = len(valid_dataset)\n",
    "\n",
    "d = full_dataset\n",
    "num_features = d.num_features\n",
    "num_classes = d[0].y.dim() if d[0].y.dim() == 1 else d[0].y.size(1)\n",
    "\n",
    "#if args.categorized:\n",
    "#    if not args.forcecats:\n",
    "#       num_classes = int(d[0].y.max().item()) + 1 if d[0].y.dim() == 1 else d[0].y.size(1)\n",
    "#    else:\n",
    "#        num_classes = args.cats\n",
    "num_classes = 4\n",
    "print ('num_classes',num_classes)\n",
    "the_weights = np.array([1., 1., 1., 1.]) #[0.017, 1., 1., 10.]\n",
    "#the_weights = np.array([1,1]) #[0.017, 1., 1., 10.]\n",
    "trainer = GNNTrainer(category_weights = the_weights, \n",
    "                     output_dir='/home/sameasy2006/hgcal_ldrd-gravnet2_wip_trainer_args/output_HGC_pion8nn', device=device)\n",
    "\n",
    "trainer.logger.setLevel(logging.DEBUG)\n",
    "strmH = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "strmH.setFormatter(formatter)\n",
    "trainer.logger.addHandler(strmH)\n",
    "\n",
    "#example lr scheduling definition\n",
    "def lr_scaling(optimizer):\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau        \n",
    "    return ReduceLROnPlateau(optimizer, mode='min', verbose=True,\n",
    "                             min_lr=5e-7, factor=0.2, \n",
    "                             threshold=0.01, patience=5)\n",
    "\n",
    "\n",
    "trainer.build_model(name='EdgeNetWithCategories', loss_func='nll_loss',\n",
    "                    optimizer='Adam', learning_rate=0.001, lr_scaling=lr_scaling,\n",
    "                    input_dim=num_features, hidden_dim=64, n_iters=6,\n",
    "                    output_dim=num_classes)\n",
    "\n",
    "trainer.print_model_summary()\n",
    "\n",
    "train_summary = trainer.train(train_loader, n_epochs, valid_data_loader=valid_loader)\n",
    "\n",
    "print(train_summary)\n",
    "    \n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--categorized', '-c', action='store_true', default=False, help='Does the model you want to train have explicit categories?')\n",
    "    parser.add_argument('--forcecats', action='store_true', default=False, help='Do we want to force the number of categories?')\n",
    "    parser.add_argument('--cats', default=1, type=int, help='Number of categories to force')\n",
    "    parser.add_argument('--optimizer', '-o', default='Adam', help='Optimizer to use for training.')\n",
    "    parser.add_argument('--model', '-m', default='EdgeNet2', help='The model to train.')\n",
    "    parser.add_argument('--loss', '-l', default='binary_cross_entropy', help='Loss function to use in training.')\n",
    "    parser.add_argument('--lr', default=0.001, type=float, help='The starting learning rate.')\n",
    "    parser.add_argument('--hidden_dim', default=64, type=int, help='Latent space size.')\n",
    "    parser.add_argument('--n_iters', default=6, type=int, help='Number of times to iterate the graph.')\n",
    "    parser.add_argument('--dataset', '-d', default='single_photon')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "'''                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join(os.environ['GNN_TRAINING_DATA_ROOT'], 'photon_hfntup_0to1000')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= [0, 1, 2, 3, 4, 5, 6]\n",
    "lrs= [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001] \n",
    "train_times= [225.54388546943665, 225.22212648391724, 225.3216323852539, 225.35716199874878, 225.40882205963135, 225.19546961784363, 225.20603489875793] \n",
    "train_losss= [0.04032796977131486, 0.015438518584319796, 0.012363892024644151, 0.008919628991402343, 0.0075437994571782635, 0.00695315887611514, 0.0066568848183646955] \n",
    "valid_times= [19.107330799102783, 19.0887508392334, 19.067117929458618, 19.119823217391968, 19.130934476852417, 19.135619401931763, 19.068397998809814]\n",
    "valid_losss= [0.017775222663031956, 0.014626435511804124, 0.010906822071867673, 0.007683055135360127, 0.006854731867877541, 0.006997309575743, 0.007357149830413887]\n",
    "valid_accs=[0.9932499189329411, 0.9944335828067422, 0.9956929219621333, 0.9973105070697365, 0.9977536107685164, 0.997650872746129, 0.9975113442820626]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(epochs,train_losss,label='training loss')\n",
    "plt.plot(epochs,valid_losss,label='validation loss')\n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,valid_accs,label='valid  acc')\n",
    "#plt.plot(epochs,valid_losss,label='validation loss')\n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([0.7523, 0.7523, 0.7496])\n",
    "b=torch.tensor([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ((a<0)==(b<0))\n",
    "c.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the_weights = np.array([0.017,1])\n",
    "\n",
    "2020-02-02 09:15:52,559 - GNNTrainer - INFO - Epoch 10\n",
    "loss = 0.00050: 100%|██████████| 11520/11520 [04:59<00:00, 38.45it/s]\n",
    "2020-02-02 09:20:52,147 - GNNTrainer - DEBUG -  Processed 11520 batches\n",
    "2020-02-02 09:20:52,148 - GNNTrainer - INFO -   Training loss: 0.00052\n",
    "2020-02-02 09:20:52,148 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
    "100%|██████████| 2880/2880 [00:28<00:00, 100.21it/s]\n",
    "2020-02-02 09:21:20,912 - GNNTrainer - DEBUG -  Processed 2880 samples in 2880 batches\n",
    "2020-02-02 09:21:20,912 - GNNTrainer - INFO -   Validation loss: 0.020 acc: 0.993\n",
    "2020-02-02 09:21:20,913 - GNNTrainer - INFO -   signal_efficiency: 0.998849 background_efficiency: 0.993102\n",
    "2020-02-02 09:21:20,913 - GNNTrainer - INFO -   signal_purity: 0.821057 background_purity: 0.999963\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "2020-02-02 10:11:51,495 - GNNTrainer - INFO - Epoch 8\n",
    "loss = 0.00047: 100%|██████████| 11520/11520 [04:59<00:00, 38.45it/s]\n",
    "2020-02-02 10:16:51,110 - GNNTrainer - DEBUG -  Processed 11520 batches\n",
    "2020-02-02 10:16:51,110 - GNNTrainer - INFO -   Training loss: 0.00052\n",
    "2020-02-02 10:16:51,111 - GNNTrainer - INFO -   Learning rate: 0.00100\n",
    "100%|██████████| 2880/2880 [00:28<00:00, 100.22it/s]\n",
    "2020-02-02 10:17:19,872 - GNNTrainer - DEBUG -  Processed 2880 samples in 2880 batches\n",
    "2020-02-02 10:17:19,873 - GNNTrainer - INFO -   Validation loss: 0.019 acc: 0.994\n",
    "2020-02-02 10:17:19,874 - GNNTrainer - INFO -   signal_efficiency: 0.998442 background_efficiency: 0.993346\n",
    "2020-02-02 10:17:19,874 - GNNTrainer - INFO -   signal_purity: 0.826232 background_purity: 0.999950\n",
    "2020-02-02 10:17:19,875 - GNNTrainer - DEBUG - Checkpointing new best model with loss: 0.01923\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
